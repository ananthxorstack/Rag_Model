# RAG System Upgrade Plan
## Modernizing with Advanced Techniques from Learning.md

---

## ðŸŽ¯ Current State Analysis

### âœ… What We Have
- **LLM**: Llama 3.2-1B via LiteLLM
- **Framework**: LangChain for RAG chains
- **Observability**: Langfuse integration (traces, generations, spans)
- **Vector DB**: ChromaDB (local, persistent)
- **Embedding**: `sentence-transformers/all-MiniLM-L6-v2` (384 dimensions)
- **Chunking**: Basic fixed-size chunking in `DocumentProcessor`
- **Retrieval**: Simple semantic search (vector similarity only)

### âŒ What We're Missing (From Learning.md)
1. **Advanced Chunking** (Semantic, Sentence-based, Overlap strategies)
2. **Hybrid Search** (Vector + BM25 keyword search)
3. **Query Rewriting** (LLM-powered query expansion)
4. **Reranking** (Cross-encoder for result refinement)
5. **Caching Layer** (Redis for embeddings/queries)
6. **Production Resilience** (Fallback mechanisms, error handling)
7. **Advanced Monitoring** (Latency tracking, quality metrics)

---

## ðŸ“‹ Upgrade Roadmap

### Phase 1: Enhanced Chunking (Week 1)
**Goal**: Implement intelligent document splitting strategies

#### Tasks
1. **Install Dependencies**
   ```bash
   pip install spacy langchain-text-splitters
   python -m spacy download en_core_web_sm
   ```

2. **Create Advanced Chunking Service** (`src/services/chunking_service.py`)
   - Implement `RecursiveCharacterTextSplitter` (LangChain)
   - Implement `SpacyTextSplitter` (sentence-aware)
   - Add configurable overlap (50-100 chars)
   - Support multiple strategies via config

3. **Update DocumentProcessor**
   - Replace current chunking with new service
   - Add chunk metadata (source, page, strategy used)
   - Implement validation (test with sample queries)

4. **Configuration** (`.env`)
   ```env
   CHUNK_SIZE=500
   CHUNK_OVERLAP=100
   CHUNK_STRATEGY=recursive  # recursive|sentence|semantic
   ```

**Expected Improvement**: 15-25% better context preservation

---

### Phase 2: Hybrid Search (Week 2)
**Goal**: Combine semantic + keyword search for better recall

#### Tasks
1. **Install BM25 Library**
   ```bash
   pip install rank-bm25
   ```

2. **Extend VectorStore** (`src/services/vector_store.py`)
   - Add BM25 index alongside vector index
   - Implement `hybrid_search()` method
   - Combine scores using weighted fusion (0.7 semantic + 0.3 BM25)

3. **Update RAG Chain** (`src/rag/chain.py`)
   - Replace pure vector retrieval with hybrid
   - Add metadata to track search method used

4. **Langfuse Tracking**
   - Log BM25 scores vs Vector scores
   - Track which method contributed most to final results

**Expected Improvement**: 20-30% better retrieval for keyword-heavy queries

---

### Phase 3: Query Rewriting (Week 3)
**Goal**: LLM reformulates vague queries into search-friendly versions

#### Tasks
1. **Create Query Rewriter** (`src/rag/rewriter.py`)
   - Use Llama 3.2-1B to expand/clarify queries
   - Prompt: "Rewrite this question to be more specific for document search"
   - Generate 1-3 alternative phrasings

2. **Update RAG Chain**
   - Add rewriting step before retrieval
   - Search with original + rewritten queries
   - Deduplicate results

3. **Langfuse Tracking**
   - Log original vs rewritten query
   - Track if rewriting improved results (via user feedback)

**Expected Improvement**: 25-40% better handling of vague/ambiguous queries

---

### Phase 4: Reranking (Week 4)
**Goal**: Use cross-encoder to re-score top results

#### Tasks
1. **Install Cross-Encoder**
   ```bash
   pip install sentence-transformers
   ```

2. **Create Reranker** (`src/rag/reranker.py`)
   - Use `cross-encoder/ms-marco-MiniLM-L-6-v2`
   - Fetch top 20 candidates from hybrid search
   - Rerank to get best 5 for LLM context

3. **Update RAG Chain**
   - Add reranking step after retrieval
   - Pass only top-K reranked chunks to LLM

4. **Langfuse Tracking**
   - Log reranking scores
   - Compare pre/post reranking quality

**Expected Improvement**: 30-50% better relevance of retrieved chunks

---

### Phase 5: Caching Layer (Week 5)
**Goal**: Add Redis for query/embedding caching

#### Tasks
1. **Install Redis**
   ```bash
   pip install redis
   # Install Redis server (Windows: https://github.com/microsoftarchive/redis/releases)
   ```

2. **Create Cache Service** (`src/services/cache_service.py`)
   - Query cache: Hash(query) â†’ cached answer
   - Embedding cache: Hash(text) â†’ cached vector
   - TTL: 1 hour for queries, 24 hours for embeddings

3. **Update Services**
   - Wrap `llm_service.generate_response()` with cache check
   - Wrap `llm_service.get_embedding()` with cache check

4. **Langfuse Tracking**
   - Add metadata: `cache_hit: true/false`
   - Track cache hit rate

**Expected Improvement**: 80-95% latency reduction for repeated queries

---

### Phase 6: Production Resilience (Week 6)
**Goal**: Implement fallback mechanisms and error handling

#### Tasks
1. **Create Resilience Wrapper** (`src/services/resilience.py`)
   ```python
   def robust_rag_pipeline(query):
       try:
           return full_rag_pipeline(query)
       except VectorDBError:
           return fallback_keyword_search(query)
       except LLMError:
           return fallback_return_chunks(query)
       except Exception:
           return "Service temporarily unavailable"
   ```

2. **Add Monitoring**
   - Track error rates by type
   - Log to Langfuse with `level="ERROR"`
   - Set up alerts for high error rates

3. **Implement Rate Limiting**
   - Protect LiteLLM API from overload
   - Queue requests if needed

**Expected Improvement**: 99.5% uptime, graceful degradation

---

### Phase 7: Advanced Langfuse Integration (Week 7)
**Goal**: Full observability with scores and custom metrics

#### Tasks
1. **Add Quality Scoring**
   - User feedback buttons (ðŸ‘/ðŸ‘Ž)
   - Log to Langfuse via `langfuse_service.score()`
   - Track: `accuracy`, `relevance`, `helpfulness`

2. **Add Custom Metrics**
   - TTFT (Time to First Token)
   - Chunk relevance scores
   - Reranking impact

3. **Create Langfuse Prompt Management**
   - Upload system prompt to Langfuse UI
   - Use `langfuse.get_prompt("rag-main-prompt")`
   - Version control prompts

**Expected Improvement**: Full pipeline visibility, A/B testing capability

---

## ðŸ› ï¸ Implementation Priority

### High Priority (Do First)
1. âœ… **Phase 1: Enhanced Chunking** - Foundation for everything else
2. âœ… **Phase 2: Hybrid Search** - Biggest accuracy boost
3. âœ… **Phase 4: Reranking** - High ROI for effort

### Medium Priority (Do Next)
4. âœ… **Phase 3: Query Rewriting** - Handles edge cases
5. âœ… **Phase 5: Caching** - Production performance

### Low Priority (Nice to Have)
6. âœ… **Phase 6: Resilience** - For production deployment
7. âœ… **Phase 7: Advanced Observability** - Continuous improvement

---

## ðŸ“Š Expected Overall Impact

| Metric | Current | After Upgrades | Improvement |
|:---|:---|:---|:---|
| **Retrieval Accuracy** | 60-70% | 85-95% | +25-35% |
| **Query Latency (cached)** | 2-3s | 0.2-0.5s | -80% |
| **Query Latency (uncached)** | 2-3s | 1.5-2.5s | -20% |
| **Handling Vague Queries** | Poor | Good | +40% |
| **System Uptime** | 95% | 99.5% | +4.5% |

---

## ðŸ”§ Tech Stack (Unchanged)

- **LLM**: Llama 3.2-1B (via LiteLLM) âœ…
- **Framework**: LangChain âœ…
- **Observability**: Langfuse âœ…
- **Vector DB**: ChromaDB âœ…

### New Additions
- **BM25**: `rank-bm25` library
- **Reranker**: `cross-encoder/ms-marco-MiniLM-L-6-v2`
- **Chunking**: `spaCy` + LangChain splitters
- **Caching**: Redis

---

## ðŸ“ Configuration Changes

### Updated `.env`
```env
# Existing
LANGFUSE_ENABLED=true
LLM_MODEL=llama-3.2-1b
EMBEDDING_MODEL=nomic-embed-text

# New
CHUNK_SIZE=500
CHUNK_OVERLAP=100
CHUNK_STRATEGY=recursive
HYBRID_SEARCH_ENABLED=true
HYBRID_SEMANTIC_WEIGHT=0.7
HYBRID_BM25_WEIGHT=0.3
RERANKING_ENABLED=true
RERANKING_TOP_K=20
QUERY_REWRITING_ENABLED=true
CACHE_ENABLED=true
REDIS_HOST=localhost
REDIS_PORT=6379
```

---

## ðŸš€ Getting Started

### Step 1: Install New Dependencies
```bash
pip install rank-bm25 spacy redis sentence-transformers
python -m spacy download en_core_web_sm
```

### Step 2: Start Redis (if using caching)
```bash
# Windows: Download and run Redis server
# Or use Docker: docker run -d -p 6379:6379 redis
```

### Step 3: Run Upgrade Script
```bash
# We'll create this to automate the migration
python scripts/upgrade_to_advanced_rag.py
```

---

## âœ… Success Criteria

After all phases:
- [ ] Retrieval accuracy > 85% on test queries
- [ ] Average latency < 2s (uncached), < 0.5s (cached)
- [ ] All traces visible in Langfuse with proper metadata
- [ ] System handles 100+ concurrent users
- [ ] Graceful degradation on component failures
- [ ] A/B testing capability via Langfuse prompts

---

## ðŸ“š References
- Learning.md Sections: 2, 3, 4, 7, 8, 9
- Current Architecture: ARCHITECTURE.md
- LangChain Docs: https://python.langchain.com/docs/modules/data_connection/
- Langfuse Docs: https://langfuse.com/docs
